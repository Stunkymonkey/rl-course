\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{geometry}
\geometry{ left=2cm, right=2cm, top=2cm, bottom=2cm, bindingoffset=5mm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\date{}
\author{}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{Felix BÃ¼hler - 2973140\\ Jan Leusmann - 2893121\\  Jamie Ullerich - 3141241}
\fancyhead[L]{Reinforcement Learning \\ SS 2020}
\renewcommand{\headrulewidth}{0.5pt}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{subcaption}
\usepackage{array}
\usepackage{bbold}
\usepackage{listings}

\title{\textbf{Exercise 2}}

\begin{document}
\maketitle 
\thispagestyle{fancy}

\section*{Task 1 - Formulating Problems}

\begin{enumerate}
	\item[a)] 
	The states would be the configuration of the chess pieces on the board. 
	These are discrete and finite states, but not every sate can be reached sometimes, depending on the current configuration of pieces.
	The actions are the specific actions which all the pieces in the current state can perform. 
	These are discrete and finite, as well. 
	For rewards, it would be good to punish when a piece is removed and reward actions which lead to check or other movements which may lead to winning.
	It is important, that there is a great reward for actual winning to avoid that the agent finds a way to for example only remove pieces and instead optimises achieving the real goal of winning. 
	\item[b)] 
	As states, we would use the position of the arm. 
	These are therefore discrete and finite.
	This way, the actions would be the movement of the arm, which are finite as well. 
	A positive reward should be given if the robot can pick objects and performs fast, this means taking a long time or dropping objects would be punished with a negative reward. 
	\item[c)] 
	There could be two discrete states, stable and unstable. 
	These indicate whether the drone must be stabilised or not. 
	As actions, the movement of the drone, e.g. left, right, up, down, forward, back could be used. 
	These are finite and can be continuous since every position is possible. 
	A positive reward can be given if the drone flies balanced and a negative one if it needs a lot of time to balance out. 
	\item[d)] 
	Another example could be the development of analogue films. 
	Here are the states data of sensors, because when developing films, certain chemicals have to be kept at an exact temperature for a certain time. 
	As actions, temperature or other factors can be changed. 
	These would be finite on a continuous scale. 
	A reward is positive if the film is successfully developed or negative if the film is damaged by the wrong settings. 
	
\end{enumerate}

\section*{Task 2 - Value Functions}

\begin{enumerate}
	\item[1.]
	
	\item[2.]
	
	Definitions given in lecture: \\
	$v_\pi (s) = \sum_{a}\pi (a|s) \sum_{s',r}p(s',r|s,a)[r+ \gamma v_\pi (s')] \ \ \forall s \in S$ \\
	$q_\pi (s,a) = \sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi (s')]$ \\
	$\rightarrow $ insert $q_\pi (s,a)$ into $v_\pi (s)$: \\ 
	$v_\pi (s) = \sum_{a}\pi (a|s) q_\pi (s,a)$
	\item[3.]
	$v_\pi (s) = \sum_{a}\pi (a|s) \sum_{s'}p(s'|s,a)[r(s,a,s')+ \gamma v_\pi (s')]$ \\
\end{enumerate}

\section*{Task 3 - Bruteforce the Policy Space}
\subsection*{a)}
$ |policies|  =  |S| * |A| $
\subsection*{b)}

\begin{lstlisting}
Value function for policy_left (always going left):
[0.         0.         0.53691275 0.         0.         1.47651007
0.         0.         5.        ]
Value function for policy_right (always going right):
[0.41401777 0.77456266 1.31147541 0.36398621 0.8185719  2.29508197
0.13235862 0.         5.        ]
\end{lstlisting}

\subsection*{c)}


\subsection*{d)}

\end{document}
